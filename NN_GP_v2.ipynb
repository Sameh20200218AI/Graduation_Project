{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YCqhS5yjfePO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import scale,OneHotEncoder\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## make standerdization for the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aQT-g6Av13uG"
      },
      "outputs": [],
      "source": [
        "def standerdize(data):\n",
        "    mean = np.mean(data, axis=1, keepdims=True)\n",
        "    std = np.std(data, axis=1, keepdims=True)\n",
        "    data_standerdized = (data - mean)/std\n",
        "    return data_standerdized"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## implement the functions that will needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YMkl0sYh2Dj5"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return(1/(1+np.exp(-z)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(z):\n",
        "    return ( ( np.exp(z) - np.exp(-z) )/( np.exp(z) + np.exp(-z) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def derivative_tanh(a):\n",
        "    return ( 1 - ( tanh(a) )**2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ANF_rALDCxnX"
      },
      "outputs": [],
      "source": [
        "# Define derivative of activation function\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s4l21UsH2JY6"
      },
      "outputs": [],
      "source": [
        "# Define mean squared error (MSE) loss function\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GLeTPSnCBFt1"
      },
      "outputs": [],
      "source": [
        "# Define derivative of MSE loss function\n",
        "def mse_loss_prime(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true) / y_pred.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yfX2HbGa2bS5"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UXnFKun_2p--"
      },
      "outputs": [],
      "source": [
        "def one_hot(y):\n",
        "  sampels=y.shape[0]\n",
        "  label=set()\n",
        "\n",
        "  for i in range(sampels):\n",
        "    label.add(y[i])\n",
        "\n",
        "  classes=len(label)\n",
        "  new_y=np.zeros((sampels ,classes))\n",
        "  for i in range(sampels):\n",
        "    new_y[i,y[i]]=1\n",
        "  \n",
        "  return new_y,classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "igAPD4b9daYN"
      },
      "outputs": [],
      "source": [
        "def soft_max(z):\n",
        "  n_sampels,n_classes=z.shape\n",
        "  new_z=np.zeros((n_sampels,n_classes))\n",
        "  for i in range(n_sampels):\n",
        "    sum=0\n",
        "    for j in range(n_classes):\n",
        "      sum+=np.exp(z[i,j])\n",
        "    for j in range(n_classes):\n",
        "      new_z[i,j]=np.exp(z[i,j])/sum\n",
        "\n",
        "  return new_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_U98BRWjUhga"
      },
      "outputs": [],
      "source": [
        "# this function to make 1 in the class that have the hightest prop from the soft max and 0 in the others classes\n",
        "def mrg_classes_of_y (y_soft):\n",
        "  num_of_samples,num_classes = y_soft.shape\n",
        "  y_pred=np.zeros(num_of_samples)\n",
        "  for i in range(num_of_samples):\n",
        "    max_value = np.max(y_soft[i])\n",
        "    index = 0\n",
        "    for ii in range(num_classes):\n",
        "      if(max_value == y_soft[i][ii]):\n",
        "        index = ii\n",
        "    y_pred[i]= index\n",
        "  return y_pred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create the NN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dP7K9MsM_qi8"
      },
      "outputs": [],
      "source": [
        "# this class is to the layers \n",
        "class L:\n",
        "  # the constractor take the two deerction the first is the input of the layer ,and the second is the output of the layer\n",
        "  def __init__(self, derc1, derc2):\n",
        "        # the weights and the bias in the initialization of the layer will take a random values \n",
        "        self.W = np.random.randn(derc1, derc2)\n",
        "        np.random.seed(0)\n",
        "\n",
        "        self.B = np.zeros((1, derc2))\n",
        "  # in this function we calculate the y_prodect and pass in into the activation function to classify the image\n",
        "  def calculate_Z(self,X):\n",
        "    self.Z = np.dot(X, self.W) + self.B\n",
        "    self.A = sigmoid(self.Z)\n",
        "    \n",
        "    #self.A = tanh(self.Z)\n",
        "    return self.A\n",
        "\n",
        "\n",
        " # this function is to update values of the weights and the bias \n",
        "  def update(self,dw,db,lr=0.001):\n",
        "    \n",
        "    self.W = self.W - lr * dw\n",
        "    self.B = self.B - lr * db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DjftP_lY9s_U"
      },
      "outputs": [],
      "source": [
        "class NN :\n",
        "\n",
        "  def __init__(self,x, y ,num_of_layers, size_of_layer, epochs = 100 , learning_rate = 0.001):\n",
        "    self.num_of_layers = num_of_layers +1\n",
        "    self.input_size = x.shape[1]\n",
        "    self.hidden_size = size_of_layer\n",
        "    # we apply the one hot encodin in the true label \n",
        "    self.y_one_hot , self.num_of_classes= one_hot(y)\n",
        "    self.output_size = self.num_of_classes\n",
        "    # we make an array of layers\n",
        "    self.layers = np.empty(self.num_of_layers, dtype=object)\n",
        "    \n",
        "    for i in range(self.num_of_layers):\n",
        "      # if this in the first layer then we will create layer with the first dirc is the size of the X and the second dirc is the size of the first hidden layer\n",
        "      if (i == 0):\n",
        "        self.layers[i] = L(self.input_size,self.hidden_size[i])\n",
        "       # if this in the last layer then we will create layer with the first dirc is the size of the last hidden layer and the second dirc is the size of classes in the true label\n",
        "      elif (i == self.num_of_layers-1):\n",
        "        self.layers[i] = L(self.hidden_size[i-1],self.output_size)\n",
        "       # if not one of the above then the we will create layer with the first dirc is the size of the previous layer output and the second dirc is the size of the this hidden layer\n",
        "      else:\n",
        "        self.layers[i] = L(self.hidden_size[i-1],self.hidden_size[i])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # the forward pass operation \n",
        "      for i in range(self.num_of_layers):\n",
        "        # if it is the first layer then the input of the layer is X\n",
        "        if (i == 0):\n",
        "          A = self.layers[i].calculate_Z(x)\n",
        "         # if it isn`t the first layer then the input of the layer is the output of the previous layer\n",
        "        else:\n",
        "          A = self.layers[i].calculate_Z(A)\n",
        "\n",
        "      # the back propaagation procces\n",
        "      if epoch != epochs-1:\n",
        "          for i in range(self.num_of_layers):\n",
        "            position = self.num_of_layers-1-i\n",
        "            # if it is the last layer then we will take the def of the cost function in our update\n",
        "            if position == self.num_of_layers-1:\n",
        "\n",
        "              # take the def of the cost function of the y_hat of the last layer and the y_true with the one hot encoding\n",
        "              sub_up =  mse_loss_prime(self.layers[position].A , self.y_one_hot)\n",
        "              # then we multiply it with the value of the input of the layer\n",
        "              dw= np.dot(np.transpose( self.layers[position-1].A ), sub_up)\n",
        "              db=np.sum(sub_up, axis=0)\n",
        "              # then we update the W and B\n",
        "              self.layers[position].update(dw,db,learning_rate)\n",
        "             \n",
        "            elif position == 0:\n",
        "              # if it is the first layer then we take the privaous sub_up with the dif of the activation function of the Z of the currect layer\n",
        "              sub_up= np.dot(sub_up , np.transpose(self.layers[position+1].W)) * sigmoid_prime(self.layers[position].Z)\n",
        "              #sub_up= np.dot(sub_up , np.transpose(self.layers[position+1].W)) * derivative_tanh(self.layers[position].Z)\n",
        "              # then we multiply it with the value of the input matrix\n",
        "              dw= np.dot(np.transpose(x), sub_up)\n",
        "              db = np.sum(sub_up, axis=0)\n",
        "              # then we update the W and B\n",
        "              self.layers[position].update(dw,db,learning_rate)\n",
        "             \n",
        "            else:              \n",
        "              # if not  the first layer  nor the last layer then we take the privaous sub_up with the dif of the activation function of the Z of the currect layer\n",
        "              sub_up= np.dot( sub_up, np.transpose(self.layers[position+1].W) ) * sigmoid_prime(self.layers[position].Z)\n",
        "              #sub_up= np.dot( sub_up, np.transpose(self.layers[position+1].W) ) * derivative_tanh(self.layers[position].Z)\n",
        "              # then we multiply it with the value of the input of the layer\n",
        "              dw = np.dot(np.transpose(self.layers[position-1].A), sub_up)\n",
        "              db = np.sum(sub_up, axis=0)\n",
        "              # then we update the W and B\n",
        "              self.layers[position].update(dw,db,learning_rate)\n",
        "             \n",
        "  # the test function is to bulit the network and then update it\n",
        "  def test (self,x_test,y_test):\n",
        "    for i in range(self.num_of_layers):\n",
        "      if (i == 0):\n",
        "        A = self.layers[i].calculate_Z(x_test)\n",
        "        \n",
        "      else:\n",
        "        A = self.layers[i].calculate_Z(A)\n",
        "    # we make soft max to the result of the network to compare it with the true labels    \n",
        "    a = soft_max(A)\n",
        "    y_pred = mrg_classes_of_y(a)\n",
        "    \n",
        "\n",
        "    print(\"y_pred\",y_pred)\n",
        "    print(\"y_test\",y_test)\n",
        "    print(accuracy(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 882,
      "metadata": {},
      "outputs": [],
      "source": [
        "training = pd.read_csv('Robot_train.csv')\n",
        "#testing = pd.read_csv('Copy_Testing _Data.csv',skiprows=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 883,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        -1\n",
            "1        -1\n",
            "2        -1\n",
            "3        -1\n",
            "4        -1\n",
            "         ..\n",
            "18345    17\n",
            "18346    17\n",
            "18347    17\n",
            "18348    17\n",
            "18349    17\n",
            "Name: label, Length: 18350, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(training.iloc[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 884,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(testing.iloc[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 885,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle training data\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "training = shuffle(training)\n",
        "training = training.reset_index(drop= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 886,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y = training.pop('label')\n",
        "\n",
        "#training.pop('X')\n",
        "#training.pop('Y')\n",
        "training.pop('X*Y')\n",
        "training.pop('X*Y+Z')\n",
        "training.pop('X+Y')\n",
        "training.pop('X+Z')\n",
        "training.pop('X-Y')\n",
        "#training.pop('X-Z')\n",
        "#training.pop('X+Y-Z')\n",
        "training.pop('X-Y-Z')\n",
        "training.pop('X/Y')\n",
        "training.pop('X/Z')\n",
        "training.pop('Y/X')\n",
        "training.pop('Y/Z')\n",
        "\n",
        "training.pop('Z/Y')\n",
        "training.pop('Z/X')\n",
        "training.pop('Y-Z')\n",
        "#training.pop('Y+Z')\n",
        "#training.pop('Y*Z+X')\n",
        "#training.pop('X*Z+Y')\n",
        "#training.pop('X*Y*Z')\n",
        "#training.pop('Y*Z')\n",
        "#training.pop('X*Z')\n",
        "#training.pop('Z')\n",
        "x = training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 887,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 888,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#print(x_test.shape,y_train.shape)\n",
        "#print(x_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 889,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(x_train.shape,y_train.shape)\n",
        "#print(x_test.shape,y_test.shape)\n",
        "#print(set(training.iloc[:,0]))\n",
        "#print(set(testing.iloc[:,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 890,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train= scale(x_train)\n",
        "y_train= np.array(y_train)\n",
        "\n",
        "X_test= scale(x_test)\n",
        "y_test= np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 891,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_pred [17. 11. 16. ... 14.  8.  9.]\n",
            "y_test [ 2 -1 16 ... 14 -1  0]\n",
            "66.07629427792916\n"
          ]
        }
      ],
      "source": [
        "model = NN(X_train,y_train,3,[20,15,10],15000,0.1)\n",
        "model.test(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sameh\\AppData\\Local\\Temp\\ipykernel_18196\\4144518951.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return(1/(1+np.exp(-z)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_pred [15.  8.  8. ...  3. 17.  2.]\n",
            "y_test [16  1  1 ... 10  6  0]\n",
            "5.110430436885378\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('Robot_train.csv')\n",
        "test = pd.read_csv('Robot_test.csv')\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "train = shuffle(train)\n",
        "test = shuffle(test)\n",
        "\n",
        "y_train = train.pop('label')\n",
        "x_train = train\n",
        "y_test = test.pop('label')\n",
        "x_test = test\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "x_train = scale(x_train)\n",
        "#x_test = scale(x_test)\n",
        "\n",
        "m = NN(x_train,y_train,3,[20,15,10],5000,0.1)\n",
        "m.test(x_test,y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
